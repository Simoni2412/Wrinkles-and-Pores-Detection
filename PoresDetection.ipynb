{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyOxnC0tf1lu0hFM3rUfNO4o",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Simoni2412/Wrinkles-and-Pores-Detection/blob/main/PoresDetection.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Install a specific, compatible version of OpenCV\n",
        "!pip install opencv-contrib-python-headless==4.8.0.74\n",
        "# Install a specific, compatible version of numpy\n",
        "# !pip install numpy==1.24.3\n",
        "\n",
        "!pip install --upgrade --force-reinstall mediapipe==0.10.5"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "0YSfiYq_XLiT",
        "outputId": "6d62514d-c84b-4249-eaad-acca3e12d956"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "PIL",
                  "cycler",
                  "dateutil",
                  "google",
                  "kiwisolver",
                  "matplotlib",
                  "mpl_toolkits",
                  "numpy",
                  "packaging",
                  "pyparsing",
                  "six"
                ]
              },
              "id": "d7203708a2804182b201ded11b1b37bf"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NJjLnXDGOAM1",
        "outputId": "14e22120-75b5-4869-a55a-3aa90ebd0c7a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import os\n",
        "import hashlib\n",
        "\n",
        "def file_hash(filepath):\n",
        "    with open(filepath, \"rb\") as f:\n",
        "        return hashlib.md5(f.read()).hexdigest()\n",
        "\n",
        "def compare_folders(folder1, folder2):\n",
        "    mismatches = []\n",
        "    for filename in os.listdir(folder1):\n",
        "        path1 = os.path.join(folder1, filename)\n",
        "        path2 = os.path.join(folder2, filename)\n",
        "\n",
        "        if not os.path.exists(path2):\n",
        "            mismatches.append((filename, \"Missing in folder2\"))\n",
        "            continue\n",
        "\n",
        "        # if file_hash(path1) != file_hash(path2):\n",
        "        #     mismatches.append((filename, \"Different content\"))\n",
        "\n",
        "    return mismatches\n",
        "\n",
        "# Example usage\n",
        "folder1 = \"/content/drive/MyDrive/Dataset /Originals\"\n",
        "folder2 = \"/content/drive/MyDrive/Dataset /Pores_Masks\"\n",
        "differences = compare_folders(folder1, folder2 )\n",
        "\n",
        "if differences:\n",
        "    for name, issue in differences:\n",
        "        print(f\"{name}: {issue}\")\n",
        "else:\n",
        "    print(\"All images are identical.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qDfkGYsJVdHy",
        "outputId": "67649f71-2456-4f76-8b37-ef46e534fe31"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "All images are identical.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers import Conv2D, Conv2D, UpSampling2D, BatchNormalization, ReLU, Multiply, Input, GlobalAveragePooling2D, Dense, Activation\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Layer, MaxPooling2D, Dropout, Add, ZeroPadding2D\n",
        "from tensorflow.keras.layers import Concatenate as Concat\n",
        "from google.colab.patches import cv2_imshow\n",
        "from tensorflow.keras import layers, models\n",
        "from PIL import Image\n",
        "import os"
      ],
      "metadata": {
        "id": "cln5UqavVfbd"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import mediapipe as mp"
      ],
      "metadata": {
        "id": "kq7iUquBFTf4"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Assuming necessary imports (cv2, numpy, mediapipe, etc.) are present\n",
        "# Assuming mp_face_mesh is initialized globally\n",
        "# Assuming get_landmark_coords is defined\n",
        "\n",
        "# Define landmark indices for eye regions (using outer contours)\n",
        "\n",
        "BUTTERFLY_ZONE_INDICES = [111, 117, 119, 120, 121, 128, 122, 6, 351, 357, 350, 349, 348, 347, 346, 345, 352, 376, 433, 416, 434, 432, 410, 423, 278, 344, 440, 275, 4, 45, 220, 115, 48, 203, 186,\n",
        "                  186, 212, 214, 192, 123, 116]\n",
        "\n",
        " # Initialize MediaPipe Face Mesh (keep this global)\n",
        "mp_face_mesh = mp.solutions.face_mesh\n",
        "\n",
        "def detect_landmarks(image):\n",
        "    \"\"\" Detect facial landmarks using Mediapipe \"\"\"\n",
        "    with mp_face_mesh.FaceMesh(static_image_mode=True, max_num_faces=1) as face_mesh:\n",
        "        results = face_mesh.process(cv2.cvtColor(image, cv2.COLOR_BGR2RGB))\n",
        "        if results.multi_face_landmarks:\n",
        "            landmarks = results.multi_face_landmarks[0]\n",
        "            return landmarks\n",
        "    return []\n",
        "\n",
        "# get_landmark_coords function remains as you provided it\n",
        "def get_landmark_coords(image, landmarks, indexes):\n",
        "    \"\"\"Extracts pixel coordinates for given landmark indexes.\"\"\"\n",
        "    h, w = image.shape[:2]\n",
        "    # Ensure indexes are within bounds\n",
        "    valid_indexes = [i for i in indexes if i is not None and 0 <= i < len(landmarks)]\n",
        "    return np.array([(int(landmarks[i].x * w), int(landmarks[i].y * h)) for i in valid_indexes], np.int32)\n",
        "\n",
        "\n",
        "def segment_facial_regions(image, landmarks):\n",
        "    \"\"\" Generate segmented facial region using a mask \"\"\"\n",
        "    h, w = image.shape[:2]\n",
        "    #eye_pts = get_landmark_coords(image, landmarks.landmark, EYE_LANDMARK_INDICES)\n",
        "\n",
        "    butterfly_pts = get_landmark_coords(image, landmarks.landmark, BUTTERFLY_ZONE_INDICES)\n",
        "\n",
        "    # Create mask\n",
        "    mask = np.zeros((h, w), dtype=np.uint8)\n",
        "    cv2.fillPoly(mask, butterfly_pts, color=(255, 255, 255)) # Eye region\n",
        "\n",
        "    \"\"\" Generate segmented facial region using a mask \"\"\"\n",
        "    #mask = np.zeros(image.shape[:2], dtype=np.uint8)\n",
        "    segmented = cv2.bitwise_and(image, image, mask=mask)\n",
        "    print(segmented.shape)\n",
        "    return segmented\n",
        "\n",
        "def different_zones(image, landmarks):\n",
        "    \"\"\" Generate segmented facial region using a mask \"\"\"\n",
        "    h, w = image.shape[:2]\n",
        "\n",
        "    # Convert landmark coordinates to pixel positions\n",
        "    # eye_pts = get_landmark_coords(image, landmarks.landmark, EYE_LANDMARK_INDICES)\n",
        "    butterfly_pts = get_landmark_coords(image, landmarks.landmark, BUTTERFLY_ZONE_INDICES)\n",
        "\n",
        "    # Initialize blank masks for each region\n",
        "\n",
        "    # eye_mask = np.zeros((h, w), dtype=np.uint8)\n",
        "    butterfly_mask = np.zeros((h, w), dtype=np.uint8)\n",
        "\n",
        "\n",
        "    # Fill masks with corresponding regions\n",
        "    #cv2.fillPoly(eye_mask, [np.array(eye_pts, dtype=np.int32)], 255)  # Eye region\n",
        "    cv2.fillPoly(butterfly_mask, [np.array(butterfly_pts, dtype=np.int32)], 255)  # Left eye region\n",
        "\n",
        "    # Extract segmented images using individual masks\n",
        "\n",
        "    butterfly_segment = cv2.bitwise_and(image, image, mask=butterfly_mask)\n",
        "\n",
        "    return butterfly_segment\n"
      ],
      "metadata": {
        "id": "HytJqzHAgeZz"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "import numpy as np\n",
        "import os\n",
        "\n",
        "# Assume detect_landmarks and get_landmark_coords are already defined\n",
        "# BUTTERFLY_ZONE_INDICES is already defined\n",
        "\n",
        "def crop_to_butterfly_zone(image, landmarks, indices):\n",
        "    h, w = image.shape[:2]\n",
        "    butterfly_pts = get_landmark_coords(image, landmarks.landmark, indices)\n",
        "    if butterfly_pts.size == 0:\n",
        "        return None, None, None, None\n",
        "\n",
        "    # Create full-size mask\n",
        "    mask = np.zeros((h, w), dtype=np.uint8)\n",
        "    cv2.fillPoly(mask, [butterfly_pts], 255)\n",
        "\n",
        "    # Find bounding box of the butterfly zone\n",
        "    ys, xs = np.where(mask > 0)\n",
        "    y_min, y_max = ys.min(), ys.max()\n",
        "    x_min, x_max = xs.min(), xs.max()\n",
        "\n",
        "    # Crop image and mask\n",
        "    cropped_image = image[y_min:y_max, x_min:x_max]\n",
        "    #cropped_mask = mask[y_min:y_max, x_min:x_max]\n",
        "\n",
        "    # For debugging/visualization\n",
        "    # cv2.imshow(\"Cropped Image\", cropped_image)\n",
        "    # cv2.imshow(\"Cropped Mask\", cropped_mask)\n",
        "    # cv2.waitKey(0)\n",
        "\n",
        "    return cropped_image, (x_min, y_min, x_max, y_max), mask\n",
        "\n",
        "# Example usage for annotation images:\n",
        "image_folder = \"/content/drive/MyDrive/Dataset /Originals\"\n",
        "input_folder = \"/content/drive/MyDrive/Dataset /Pores_annotated\"\n",
        "output_img_folder = \"/content/drive/MyDrive/Dataset /Pore_Cropped_Images\"\n",
        "output_mask_folder = \"/content/drive/MyDrive/Dataset /Pore_Cropped_Masks\"\n",
        "#output_mask_anno_folder = \"/content/drive/MyDrive/Dataset /Pore_Cropped_Masks_anno\"\n",
        "\n",
        "# os.makedirs(output_img_folder, exist_ok=True)\n",
        "# os.makedirs(output_mask_folder, exist_ok=True)\n",
        "# os.makedirs(output_mask_anno_folder, exist_ok=True)\n",
        "\n",
        "for filename in os.listdir(input_folder):\n",
        "    if filename.lower().endswith(('.png', '.jpg', '.jpeg')):\n",
        "        img_path = os.path.join(input_folder, filename)\n",
        "        image = cv2.imread(img_path)\n",
        "        landmarks = detect_landmarks(image)\n",
        "        result = crop_to_butterfly_zone(image, landmarks, BUTTERFLY_ZONE_INDICES)\n",
        "        if result is None:\n",
        "            print(f\"Could not process {filename}\")\n",
        "            continue\n",
        "        cropped_img, bbox, _ = result\n",
        "        #Convert to HSV color space for better color segmentation\n",
        "        hsv = cv2.cvtColor(cropped_img, cv2.COLOR_BGR2HSV)\n",
        "\n",
        "        # Narrow range for blue (fine-tuned)\n",
        "        lower_blue = np.array([100, 150, 100])  # H, S, V\n",
        "        upper_blue = np.array([130, 255, 255])\n",
        "\n",
        "        # Create mask for blue regions\n",
        "        mask_blue = cv2.inRange(hsv, lower_blue, upper_blue)\n",
        "\n",
        "        # Optional: Morphological closing to fill small holes\n",
        "        kernel = cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (3, 3))\n",
        "        mask_binary = cv2.morphologyEx(mask_blue, cv2.MORPH_CLOSE, kernel)\n",
        "\n",
        "        cv2.imwrite(os.path.join(output_mask_anno_folder, filename), mask_binary)\n",
        "\n",
        "for filename in os.listdir(image_folder):\n",
        "    if filename.lower().endswith(('.png', '.jpg', '.jpeg')):\n",
        "        img_path = os.path.join(image_folder, filename)\n",
        "        image = cv2.imread(img_path)\n",
        "        landmarks = detect_landmarks(image)\n",
        "        result = crop_to_butterfly_zone(image, landmarks, BUTTERFLY_ZONE_INDICES)\n",
        "        if result is None:\n",
        "            print(f\"Could not process {filename}\")\n",
        "            continue\n",
        "        cropped_img, bbox, _ = result\n",
        "        # cv2.imwrite(os.path.join(output_img_folder, filename), cropped_img)\n",
        "\n",
        "# To use the mask from annotation:\n",
        "# - The cropped mask you saved during annotation is already the correct size for the cropped region.\n",
        "# - When you crop a new original image using the same method, you can use the saved cropped mask directly.\n"
      ],
      "metadata": {
        "id": "0jEqEk6iHEE-"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "IMG_SIZE = (256, 256)\n",
        "\n",
        "def preprocess_image(image_path, mask_path, n_classes=1):\n",
        "    # Read image\n",
        "    image = cv2.imread(image_path.numpy().decode('utf-8'))\n",
        "   # --- Added check ---\n",
        "    if image is None:\n",
        "        print(f\"Error: Could not read image file at path: {image_path}\")\n",
        "        # Return default values or handle the error as needed\n",
        "        # For now, returning zero tensors to allow the dataset to build,\n",
        "        # but the root cause needs to be investigated.\n",
        "        return np.zeros((IMG_SIZE[0], IMG_SIZE[1], 3), dtype=np.float32), np.zeros((IMG_SIZE[0], IMG_SIZE[1], 1), dtype=np.float32 if n_classes==1 else np.uint8)\n",
        "    # --- End added check ---\n",
        "\n",
        "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
        "\n",
        "    # Enhancement (CLAHE + bilateral)\n",
        "    gray = cv2.cvtColor(image, cv2.COLOR_RGB2GRAY)\n",
        "    clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(8,8))\n",
        "    enhanced = clahe.apply(gray)\n",
        "    #filtered = cv2.bilateralFilter(enhanced, 9, 75, 75)\n",
        "    normalized = enhanced.astype(np.float32) / 255.0\n",
        "\n",
        "    # Convert to 3 channels\n",
        "    image = np.stack([normalized]*3, axis=-1)\n",
        "    #image = np.stack([filtered]*3, axis=-1)\n",
        "    image = cv2.resize(image, IMG_SIZE)\n",
        "\n",
        "    # Read mask\n",
        "    mask = cv2.imread(mask_path.numpy().decode('utf-8'), cv2.IMREAD_GRAYSCALE)\n",
        "    mask = cv2.resize(mask, IMG_SIZE, interpolation=cv2.INTER_NEAREST)\n",
        "    if n_classes == 1:\n",
        "        mask = (mask > 127).astype(np.float32)\n",
        "        mask = mask[..., None]\n",
        "    else:\n",
        "        # For multiclass, ensure mask values are 0, 1, 2, ...\n",
        "        mask = mask.astype(np.uint8)\n",
        "        mask = mask[..., None]\n",
        "    return image, mask\n",
        "\n",
        "def tf_preprocess_image(image_path, mask_path, n_classes=1):\n",
        "    image, mask = tf.py_function(\n",
        "        func=preprocess_image,\n",
        "        inp=[image_path, mask_path, n_classes],\n",
        "        Tout=[tf.float32, tf.float32 if n_classes==1 else tf.uint8]\n",
        "    )\n",
        "    image.set_shape([IMG_SIZE[0], IMG_SIZE[1], 3])\n",
        "    mask.set_shape([IMG_SIZE[0], IMG_SIZE[1], 1])\n",
        "\n",
        "    return image, mask\n",
        "\n",
        "def get_dataset(image_dir, mask_dir, batch_size=8):\n",
        "    image_files = sorted([os.path.join(image_dir, f) for f in os.listdir(image_dir)])\n",
        "    mask_files = sorted([os.path.join(mask_dir, f) for f in os.listdir(mask_dir)])\n",
        "    dataset = tf.data.Dataset.from_tensor_slices((image_files, mask_files))\n",
        "    dataset = dataset.map(lambda x, y: tf_preprocess_image(x, y, n_classes), num_parallel_calls=tf.data.AUTOTUNE)\n",
        "    dataset = dataset.batch(batch_size).prefetch(tf.data.AUTOTUNE)\n",
        "    return dataset\n"
      ],
      "metadata": {
        "id": "pZwJsAs8HaUM"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from typing import Concatenate\n",
        "class GaussianFilterLayer(Layer):\n",
        "    def __init__(self, **kwargs):\n",
        "        super(GaussianFilterLayer, self).__init__(**kwargs)\n",
        "\n",
        "    def call(self, x):\n",
        "        gaussian_kernel = tf.constant([[1, 2, 1], [2, 4, 2], [1, 2, 1]], dtype=tf.float32)\n",
        "        gaussian_kernel = gaussian_kernel[:, :, None, None] / 16  # Normalized kernel\n",
        "        gaussian_kernel = tf.repeat(gaussian_kernel, repeats=x.shape[-1], axis=-1)\n",
        "        return tf.nn.conv2d(x, filters=tf.cast(gaussian_kernel, tf.float32), strides=[1,1,1,1], padding=\"SAME\")\n",
        "\n",
        "def channel_avg(x):\n",
        "    return tf.reduce_mean(x, axis=-1, keepdims=True)\n",
        "\n",
        "def channel_max(x):\n",
        "    return tf.reduce_max(x, axis=-1, keepdims=True)\n",
        "\n",
        "def spatial_attention(x):\n",
        "    avg_pool =  layers.Lambda(channel_avg)(x)\n",
        "    max_pool = layers.Lambda(channel_max)(x)\n",
        "    #max_pool = tf.reduce_max(x, axis=-1, keepdims=True)\n",
        "    concat = layers.Concatenate(axis=-1)([avg_pool, max_pool])\n",
        "    sa = layers.Conv2D(1, 7, padding='same', activation='sigmoid')(concat)\n",
        "    return layers.Multiply()([x, sa])\n",
        "\n",
        "# ---- Attention Module ----\n",
        "def attention_block(x, g, inter_channels):\n",
        "    theta_x = Conv2D(inter_channels, (1,1), padding=\"same\")(x)\n",
        "    phi_g = Conv2D(inter_channels, (1,1), padding=\"same\")(g)\n",
        "    add_xg = Add()([theta_x, phi_g])\n",
        "    act_xg = Activation('relu')(add_xg)\n",
        "    psi = Conv2D(1, (1,1), padding=\"same\", activation=\"sigmoid\")(act_xg)\n",
        "    return Multiply()([x, psi])\n",
        "\n",
        "def conv_block(x, filters):\n",
        "    x = Conv2D(filters, 3, padding='same')(x)\n",
        "    x = BatchNormalization()(x)\n",
        "    x = Activation('relu')(x)\n",
        "    x = Conv2D(filters, 3, padding='same')(x)\n",
        "    x = BatchNormalization()(x)\n",
        "    x = Activation('relu')(x)\n",
        "    return x\n",
        "\n",
        "def pore_detection_model(input_shape):\n",
        "    \"\"\" Define U-Net with Spatial & Channel Attention \"\"\"\n",
        "    inputs = Input(shape=input_shape)\n",
        "    # Encoder\n",
        "    conv1 = conv_block(inputs, 32)\n",
        "    pool1 = MaxPooling2D()(conv1)\n",
        "\n",
        "    conv2 = conv_block(pool1, 64)\n",
        "    pool2 = MaxPooling2D()(conv2)\n",
        "\n",
        "    conv3 = conv_block(pool2, 128)\n",
        "    pool3 = MaxPooling2D()(conv3)\n",
        "\n",
        "    conv4 = conv_block(pool3, 256)\n",
        "    pool4 = MaxPooling2D()(conv4)\n",
        "\n",
        "    # Bottleneck\n",
        "    bneck = conv_block(pool4, 512)\n",
        "    bn = spatial_attention(bneck)\n",
        "\n",
        "    # Decoder\n",
        "    up4 = UpSampling2D()(bn)\n",
        "    attn4 = attention_block(conv4, up4, 256)\n",
        "    merge4 = Concat()([up4, attn4])\n",
        "    conv5 = conv_block(merge4, 256)\n",
        "\n",
        "    up3 = UpSampling2D()(conv5)\n",
        "    attn3 = attention_block(conv3, up3, 128)\n",
        "    merge3 = Concat()([up3, attn3])\n",
        "    conv6 = conv_block(merge3, 128)\n",
        "\n",
        "    up2 = UpSampling2D()(conv6)\n",
        "    attn2 = attention_block(conv2, up2, 64)\n",
        "    merge2 = Concat()([up2, attn2])\n",
        "    conv7 = conv_block(merge2, 64)\n",
        "\n",
        "    up1 = UpSampling2D()(conv7)\n",
        "    attn1 = attention_block(conv1, up1, 32)\n",
        "    merge1 = Concat()([up1, attn1])\n",
        "    conv8 = conv_block(merge1, 32)\n",
        "\n",
        "    # Output layer\n",
        "\n",
        "    output_layer = Conv2D(1, 1, activation=\"sigmoid\", padding=\"same\")(conv8)\n",
        "\n",
        "\n",
        "    model = Model(inputs, output_layer)\n",
        "    return model\n"
      ],
      "metadata": {
        "id": "W9bw1B9gPnyI"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "def dice_loss(y_true, y_pred, smooth=1e-6):\n",
        "    y_true_f = tf.reshape(y_true, [-1])\n",
        "    y_pred_f = tf.reshape(y_pred, [-1])\n",
        "    intersection = tf.reduce_sum(y_true_f * y_pred_f)\n",
        "    return 1 - (2. * intersection + smooth) / (tf.reduce_sum(y_true_f) + tf.reduce_sum(y_pred_f) + smooth)\n",
        "\n",
        "def combined_loss(y_true, y_pred):\n",
        "    bce = tf.keras.losses.binary_crossentropy(y_true, y_pred)\n",
        "    # focal = focal_loss(y_true, y_pred)\n",
        "    dsc = dice_loss(y_true, y_pred)\n",
        "    return bce + dsc"
      ],
      "metadata": {
        "id": "EFg7Rk9uP-4o"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "image_path = \"/content/drive/MyDrive/Dataset /Pore_Cropped_Images/\"\n",
        "mask_path = \"/content/drive/MyDrive/Dataset /Pore_Cropped_Masks_anno\"\n",
        "val_image_path = \"/content/drive/MyDrive/Dataset /Val_pores_images/\"\n",
        "val_mask_path = \"/content/drive/MyDrive/Dataset /Val_pores_mask/\"\n",
        "\n",
        "# Define early stopping callback\n",
        "early_stop = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=10)  # Adjust patience as needed\n",
        "\n",
        "n_classes = 1\n",
        "train_dataset = get_dataset(image_path, mask_path, batch_size=3)\n",
        "val_dataset = get_dataset(val_image_path, val_mask_path, batch_size=3)\n",
        "\n",
        "#model = attention_unet_paper(input_shape=(256,256,3), padding=0)\n",
        "model = pore_detection_model(input_shape=(256,256,3))\n",
        "model.compile(optimizer='adam', loss=combined_loss, metrics=['accuracy'])\n",
        "# # Define the MSE loss\n",
        "# mse_loss = tf.keras.losses.MeanSquaredError()\n",
        "\n",
        "# # Compile the model using MSE\n",
        "# model.compile(optimizer='adam', loss=mse_loss, metrics=['accuracy'])\n",
        "\n",
        "\n",
        "# Train\n",
        "model.fit(train_dataset,validation_data=val_dataset, epochs=100, callbacks=[early_stop])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TYWSl0quP4ZC",
        "outputId": "4265f9ba-a336-4f3f-ac96-c513045369dc"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n",
            "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m58s\u001b[0m 1s/step - accuracy: 0.7336 - loss: 0.8376 - val_accuracy: 0.0163 - val_loss: 666.5446\n",
            "Epoch 2/100\n",
            "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 174ms/step - accuracy: 0.8518 - loss: 0.5313 - val_accuracy: 0.0162 - val_loss: 934.5930\n",
            "Epoch 3/100\n",
            "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 136ms/step - accuracy: 0.8598 - loss: 0.5206 - val_accuracy: 0.0162 - val_loss: 26.0219\n",
            "Epoch 4/100\n",
            "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 127ms/step - accuracy: 0.8662 - loss: 0.4818 - val_accuracy: 0.0162 - val_loss: 5.7439\n",
            "Epoch 5/100\n",
            "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 131ms/step - accuracy: 0.8685 - loss: 0.4794 - val_accuracy: 0.0162 - val_loss: 4.3091\n",
            "Epoch 6/100\n",
            "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 164ms/step - accuracy: 0.8724 - loss: 0.4662 - val_accuracy: 0.0162 - val_loss: 4.1939\n",
            "Epoch 7/100\n",
            "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 126ms/step - accuracy: 0.8739 - loss: 0.4626 - val_accuracy: 0.0162 - val_loss: 4.1833\n",
            "Epoch 8/100\n",
            "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 122ms/step - accuracy: 0.8737 - loss: 0.4583 - val_accuracy: 0.0162 - val_loss: 3.1298\n",
            "Epoch 9/100\n",
            "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 121ms/step - accuracy: 0.8778 - loss: 0.4501 - val_accuracy: 0.0175 - val_loss: 3.6261\n",
            "Epoch 10/100\n",
            "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 123ms/step - accuracy: 0.8784 - loss: 0.4449 - val_accuracy: 0.0179 - val_loss: 3.2015\n",
            "Epoch 11/100\n",
            "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 168ms/step - accuracy: 0.8760 - loss: 0.4507 - val_accuracy: 0.0554 - val_loss: 2.8591\n",
            "Epoch 12/100\n",
            "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 127ms/step - accuracy: 0.8774 - loss: 0.4565 - val_accuracy: 0.0263 - val_loss: 3.4685\n",
            "Epoch 13/100\n",
            "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 130ms/step - accuracy: 0.8805 - loss: 0.4424 - val_accuracy: 0.0434 - val_loss: 4.4496\n",
            "Epoch 14/100\n",
            "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 122ms/step - accuracy: 0.8822 - loss: 0.4349 - val_accuracy: 0.0440 - val_loss: 4.2300\n",
            "Epoch 15/100\n",
            "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 162ms/step - accuracy: 0.8840 - loss: 0.4247 - val_accuracy: 0.0777 - val_loss: 4.0044\n",
            "Epoch 16/100\n",
            "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 139ms/step - accuracy: 0.8830 - loss: 0.4329 - val_accuracy: 0.0232 - val_loss: 8.7168\n",
            "Epoch 17/100\n",
            "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 125ms/step - accuracy: 0.8785 - loss: 0.4438 - val_accuracy: 0.0286 - val_loss: 7.6312\n",
            "Epoch 18/100\n",
            "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 135ms/step - accuracy: 0.8850 - loss: 0.4240 - val_accuracy: 0.0299 - val_loss: 7.6636\n",
            "Epoch 19/100\n",
            "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 214ms/step - accuracy: 0.8837 - loss: 0.4229 - val_accuracy: 0.0462 - val_loss: 4.5693\n",
            "Epoch 20/100\n",
            "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 125ms/step - accuracy: 0.8831 - loss: 0.4320 - val_accuracy: 0.0724 - val_loss: 5.5661\n",
            "Epoch 21/100\n",
            "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 125ms/step - accuracy: 0.8804 - loss: 0.4375 - val_accuracy: 0.2573 - val_loss: 5.3343\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.history.History at 0x7b8f5fe55ed0>"
            ]
          },
          "metadata": {},
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.save('model_pores_batch3.h5')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iP7c3f0OSTfx",
        "outputId": "e60cd426-050a-430f-a2c6-066ccd170b83"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for images, masks in train_dataset.take(1):\n",
        "    preds = model.predict(images)\n",
        "    print(\"Prediction min:\", preds.min(), \"max:\", preds.max(), \"mean:\", preds.mean())\n",
        "    break  # Only take the first batch"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CuzfQdrrSeyp",
        "outputId": "ec24d4a8-d73a-4a92-cee0-d336e1640cff"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3s/step\n",
            "Prediction min: 5.1961294e-13 max: 1.0 mean: 0.75217986\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate model on the validation set\n",
        "results = model.evaluate(val_dataset)  # val_dataset is your tf.data.Dataset for validation\n",
        "print(\"Validation Results:\", results)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UL5ka6BSSn5s",
        "outputId": "2a14cd9c-d77f-4ae6-dfdc-a5e597077262"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step - accuracy: 0.2402 - loss: 5.4488 \n",
            "Validation Results: [5.334267616271973, 0.25731199979782104]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "for images, masks in val_dataset.take(5):  # Show 5 batches\n",
        "    preds = model.predict(images)\n",
        "    for i in range(images.shape[0]):  # For each image in the batch\n",
        "        img = images[i].numpy()\n",
        "        true_mask = masks[i].numpy().squeeze()\n",
        "        pred_mask = preds[i].squeeze()\n",
        "        # If your model outputs values in [0,1], you may want to threshold\n",
        "        pred_mask_bin = (pred_mask > 0.5).astype(np.uint8)\n",
        "\n",
        "        plt.figure(figsize=(12,4))\n",
        "        plt.subplot(1,3,1)\n",
        "        plt.imshow(img.squeeze(), cmap='gray' if img.shape[-1]==1 else None)\n",
        "        plt.title(\"Cropped Image\")\n",
        "        plt.axis('off')\n",
        "\n",
        "        plt.subplot(1,3,2)\n",
        "        plt.imshow(true_mask, cmap='gray')\n",
        "        plt.title(\"True Mask\")\n",
        "        plt.axis('off')\n",
        "\n",
        "        plt.subplot(1,3,3)\n",
        "        plt.imshow(pred_mask_bin, cmap='gray')\n",
        "        plt.title(\"Predicted Mask\")\n",
        "        plt.axis('off')\n",
        "\n",
        "        plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "YoUOkopdSrQ5",
        "outputId": "b16272c3-47c6-4697-c54f-55a2d3a076af"
      },
    {
      "cell_type": "code",
      "source": [
        "# At inference time, for a new original image:\n",
        "def crop_original_image_to_butterfly(image, landmarks, indices):\n",
        "    h, w = image.shape[:2]\n",
        "    butterfly_pts = get_landmark_coords(image, landmarks.landmark, indices)\n",
        "    if butterfly_pts.size == 0:\n",
        "        return None, None, None\n",
        "    mask = np.zeros((h, w), dtype=np.uint8)\n",
        "    cv2.fillPoly(mask, [butterfly_pts], 255)\n",
        "    ys, xs = np.where(mask > 0)\n",
        "    y_min, y_max = ys.min(), ys.max()\n",
        "    x_min, x_max = xs.min(), xs.max()\n",
        "    cropped_image = image[y_min:y_max, x_min:x_max]\n",
        "    bbox = (x_min, y_min, x_max, y_max)\n",
        "    return cropped_image, bbox, mask"
      ],
      "metadata": {
        "id": "A_PGbaJJHr9L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def place_mask_on_original(original_image, pred_mask, bbox):\n",
        "    x_min, y_min, x_max, y_max = bbox\n",
        "    mask_full = np.zeros(original_image.shape[:2], dtype=pred_mask.dtype)\n",
        "    mask_full[y_min:y_max, x_min:x_max] = pred_mask\n",
        "    return mask_full"
      ],
      "metadata": {
        "id": "fAN5asrCObcR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "mask = place_mask_on_original(image, pred_mask_bin, bbox)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 291
        },
        "id": "aT95dsePS5zV",
        "outputId": "5300a78a-4095-46fe-ac3a-3c7e641eb90b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "could not broadcast input array from shape (256,256) into shape (701,1405)",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-36-16716ed05288>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mplace_mask_on_original\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpred_mask_bin\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbbox\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-35-b045423d38c0>\u001b[0m in \u001b[0;36mplace_mask_on_original\u001b[0;34m(original_image, pred_mask, bbox)\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0mx_min\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_min\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_max\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_max\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbbox\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mmask_full\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moriginal_image\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpred_mask\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0mmask_full\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0my_min\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0my_max\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_min\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mx_max\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpred_mask\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mmask_full\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: could not broadcast input array from shape (256,256) into shape (701,1405)"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "leg-8hqZTF0j"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
